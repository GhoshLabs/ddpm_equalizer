# -*- coding: utf-8 -*-
"""diffusion_eq.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BbZSe-aYaeYJ4yhHIVktoHbU9_XAlNIS
"""

import torch
import math
import os
from channels import h, n_tx, n_rx, l_seq, M, d
import torch.nn.functional as F
import matplotlib.pyplot as plt
from diff_model import T, sample_timestep, Backbone
from dataset import Datasets
from constellation import plot_scatter

# --- Load the trained model ---
# Define device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Instantiate a new model instance
model = Backbone(T)
model = model.to(torch.double).to(device)

# Load the saved state dictionary
model_path = os.path.join('saved_models', 'ddpm_equalizer_model.pth')
model.load_state_dict(torch.load(model_path, map_location=device))
print(f"Model loaded from {model_path}")

dataset = Datasets(M, d, h, n_tx, n_rx, l_seq)
sample_x, sample_y = dataset[0] # Get a sample pair of (transmitted, received)

print("Original transmitted signal constellation:")
plot_scatter(sample_x[0:n_tx, 0:l_seq])

print("Received signal constellation (after channel distortion):")
plot_scatter(sample_y[0:n_rx, 0:l_seq])


print("\nStarting the reverse diffusion (equalization) process...")
print("The goal is to recover the original transmitted signal from the received signal.")
model.eval() # Set the model to evaluation mode

with torch.no_grad():
    # Start with pure random noise
    # The shape should match the input to the model, so we use sample_x's shape
    # and add a batch dimension of 1.
    x_t = (torch.randn_like(sample_x.real) + 1j * torch.randn_like(sample_x.imag)) / math.sqrt(2)
    x_t = x_t.unsqueeze(0).to(device)
    
    plot_scatter(x_t.squeeze(0).cpu()[0:n_tx, 0:l_seq])

    # The received signal `sample_y` is our condition. Add a batch dim and move to device.
    y_cond = sample_y.unsqueeze(0).to(device)

    for t in range(T - 1, -1, -1):
        x_t = sample_timestep(model, x_t, t, y_cond)

        # Plot constellation every 20 steps
        if t % 20 == 0:
            print(f"Constellation at timestep t={t}")
            # We need to move the tensor to CPU and remove the batch dimension for plotting
            denoised_sample_for_plot = x_t.squeeze(0).cpu()
            # We plot only the part corresponding to the original sequence, not the padding
            plot_scatter(denoised_sample_for_plot[0:n_tx, 0:l_seq])

print("Final equalized (recovered) signal constellation:")
plot_scatter(x_t.squeeze(0).cpu()[0:n_tx, 0:l_seq])

# Calculate the final MSE between the original transmitted signal and the final equalized signal
final_equalized_signal = x_t.squeeze(0).cpu()[0:n_tx, 0:l_seq]
original_signal = sample_x[0:n_tx, 0:l_seq]
final_mse = F.mse_loss(final_equalized_signal.real, original_signal.real) + F.mse_loss(final_equalized_signal.imag, original_signal.imag)

print(f"\nFinal Mean Squared Error (MSE): {final_mse.item()}")
